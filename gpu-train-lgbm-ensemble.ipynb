{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9492fa",
   "metadata": {
    "papermill": {
     "duration": 0.006713,
     "end_time": "2024-01-05T08:04:00.636534",
     "exception": false,
     "start_time": "2024-01-05T08:04:00.629821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict Prosumer Energy PatternsÂ¶\n",
    "This notebook is modified from the existing notebook [Enefit PEBOP: LGBM ensemble](https://www.kaggle.com/code/siddhvr/enefit-pebop-lgbm-ensemble)\n",
    "\n",
    "This notebook predicts electricity production and consumption for Estonian solar customers using weather, price, and PV data and train an ensemble of LightGBM models that uses decision tree based learning algorithms.\n",
    "\n",
    "**[Change logs]**\n",
    "- [version 13] (score=66.84) Include two classes to process and transform the training and testing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597dbf15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:04:00.652841Z",
     "iopub.status.busy": "2024-01-05T08:04:00.652396Z",
     "iopub.status.idle": "2024-01-05T08:04:07.499751Z",
     "shell.execute_reply": "2024-01-05T08:04:07.498384Z"
    },
    "papermill": {
     "duration": 6.858882,
     "end_time": "2024-01-05T08:04:07.502653",
     "exception": false,
     "start_time": "2024-01-05T08:04:00.643771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import enefit, torch, pickle, gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# Sklean packages...\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8b471",
   "metadata": {
    "papermill": {
     "duration": 0.006079,
     "end_time": "2024-01-05T08:04:07.515373",
     "exception": false,
     "start_time": "2024-01-05T08:04:07.509294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8904f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:04:07.530081Z",
     "iopub.status.busy": "2024-01-05T08:04:07.529427Z",
     "iopub.status.idle": "2024-01-05T08:04:33.814179Z",
     "shell.execute_reply": "2024-01-05T08:04:33.813010Z"
    },
    "papermill": {
     "duration": 26.295148,
     "end_time": "2024-01-05T08:04:33.816843",
     "exception": false,
     "start_time": "2024-01-05T08:04:07.521695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/train.csv')\n",
    "gas_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv')\n",
    "electricity_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv')\n",
    "client_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/client.csv')\n",
    "fw_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv')\n",
    "hw_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\n",
    "locations= pd.read_csv('/kaggle/input/locations/county_lon_lats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ea54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:58:24.479531Z",
     "iopub.status.busy": "2023-12-16T18:58:24.479216Z",
     "iopub.status.idle": "2023-12-16T18:58:24.646928Z",
     "shell.execute_reply": "2023-12-16T18:58:24.646014Z",
     "shell.execute_reply.started": "2023-12-16T18:58:24.479495Z"
    },
    "papermill": {
     "duration": 0.006197,
     "end_time": "2024-01-05T08:04:33.829784",
     "exception": false,
     "start_time": "2024-01-05T08:04:33.823587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Processing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94305ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:04:33.845021Z",
     "iopub.status.busy": "2024-01-05T08:04:33.844607Z",
     "iopub.status.idle": "2024-01-05T08:04:33.873107Z",
     "shell.execute_reply": "2024-01-05T08:04:33.871874Z"
    },
    "papermill": {
     "duration": 0.039186,
     "end_time": "2024-01-05T08:04:33.875526",
     "exception": false,
     "start_time": "2024-01-05T08:04:33.836340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataProcessing():\n",
    "\n",
    "    def feat_eng_train(self, data, client, hist_weather,forecast_weather, electricity, gas, locations):\n",
    "\n",
    "        #Dropping (target) nan values\n",
    "        data= data[data['target'].notnull()] \n",
    "        #Converting (datetime) column to datetime\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'], utc=True)\n",
    "        #Renaming (forecast_date) to (datetime) for merging with the train data later\n",
    "        electricity = electricity.rename(columns= {'forecast_date' : 'datetime'})\n",
    "        #Converting (datetime) column to datetime\n",
    "        electricity['datetime'] = pd.to_datetime(electricity['datetime'], utc= True)\n",
    "        #Decreasing (data_block_id) in client data because it's 2 steps ahead from train's data (data_block_id)\n",
    "        client['data_block_id'] -= 2\n",
    "\n",
    "        locations = locations.drop('Unnamed: 0', axis= 1) \n",
    "\n",
    "        #Rounding the (latitude) and (longitude) for 1 decimal fraction\n",
    "        forecast_weather[['latitude', 'longitude']] = forecast_weather[['latitude',\n",
    "                                                                        'longitude']].astype(float).round(1)\n",
    "        #Merging counties in locations data with the coordinations in the forecast_weather data\n",
    "        forecast_weather= forecast_weather.merge(locations, how='left',\n",
    "                                                 on=['longitude','latitude'])\n",
    "        #dropping nan values\n",
    "        forecast_weather.dropna(axis= 0, inplace= True)    \n",
    "        #Converting (county) column to integer\n",
    "        forecast_weather['county'] = forecast_weather['county'].astype('int64')\n",
    "        #Dropping the columns we won't need | We will use the (forecast_datetime) column instead of the (origin_datetime)\n",
    "        forecast_weather.drop(['origin_datetime', 'latitude','longitude', 'hours_ahead', 'data_block_id'], axis=1, inplace= True)\n",
    "        #Renaming (forecast_datetime) to (datetime) for merging with the train data later\n",
    "        forecast_weather.rename(columns={'forecast_datetime': 'datetime'}, inplace= True)\n",
    "        #Converting (datetime) column to datetime\n",
    "        forecast_weather['datetime']= pd.to_datetime(forecast_weather['datetime'], utc= True)\n",
    "\n",
    "        forecast_weather_datetime= forecast_weather.groupby([forecast_weather['datetime'].dt.to_period('h')])[list(forecast_weather.drop(['county','datetime'], axis= 1).columns)].mean().reset_index()\n",
    "        #After converting the (datetime) column to hour period for the groupby we convert it back to datetime\n",
    "        forecast_weather_datetime['datetime']= pd.to_datetime(forecast_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n",
    "\n",
    "        forecast_weather_datetime_county= forecast_weather.groupby(['county',forecast_weather['datetime'].dt.to_period('h')])[list(forecast_weather.drop(['county','datetime'], axis= 1).columns)].mean().reset_index()\n",
    "\n",
    "        #After converting the (datetime) column to hour period for the groupby we convert it back to datetime\n",
    "        forecast_weather_datetime_county['datetime']= pd.to_datetime(\n",
    "            forecast_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "\n",
    "        #Rounding the (latitude) and (longitude) for 1 decimal fraction           \n",
    "        hist_weather[['latitude', 'longitude']] = hist_weather[['latitude', 'longitude']].astype(float).round(1)\n",
    "\n",
    "        #Merging counties in locations data with the coordinations in the historical_weather data\n",
    "        hist_weather= hist_weather.merge(locations, how='left', on=['longitude','latitude'])    \n",
    "        #Dropping nan values\n",
    "        hist_weather.dropna(axis= 0, inplace= True)\n",
    "        #Dropping the columns we won't need\n",
    "        hist_weather.drop(['latitude', 'longitude'], axis=1, inplace= True)\n",
    "        #Converting (county) to integer\n",
    "        hist_weather['county'] = hist_weather['county'].astype('int64')\n",
    "        #Converting (datetime) column to datetime\n",
    "        hist_weather['datetime']= pd.to_datetime(hist_weather['datetime'], utc= True)\n",
    "\n",
    "        hist_weather_datetime= hist_weather.groupby([hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime','data_block_id'], axis= 1).columns)].mean().reset_index()    \n",
    "\n",
    "        #After converting the (datetime) column to hour period for the groupby we convert it back to datetime\n",
    "        hist_weather_datetime['datetime']= pd.to_datetime(hist_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n",
    "\n",
    "        #Merging (data_block_id) back after dropping it in the last step | (data_block_id will be used to merge with train data)\n",
    "        hist_weather_datetime= hist_weather_datetime.merge(\n",
    "            hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "\n",
    "        hist_weather_datetime_county= hist_weather.groupby(['county',hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime', 'data_block_id'], axis= 1).columns)].mean().reset_index() \n",
    "        #After converting the (datetime) column to hour period for the groupby we convert it back to datetime\n",
    "        hist_weather_datetime_county['datetime']= pd.to_datetime(\n",
    "            hist_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "        #Merging (data_block_id) back after dropping it in the last step\n",
    "        hist_weather_datetime_county= hist_weather_datetime_county.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "        #Adding year column in train data\n",
    "        data['year'] = data['datetime'].dt.year\n",
    "        #Adding month column in train data\n",
    "        data['month'] = data['datetime'].dt.month\n",
    "        #Adding day column in train data\n",
    "        data['day'] = data['datetime'].dt.day\n",
    "        #Adding hour column in train data\n",
    "        data['hour'] = data['datetime'].dt.hour\n",
    "        #Adding dayofweek column in train data\n",
    "        data['dayofweek'] = data['datetime'].dt.dayofweek\n",
    "        #Adding dayofyear column in train data\n",
    "        data['dayofyear']= data['datetime'].dt.dayofyear\n",
    "        #Adding hour column to electricity used to merge with the train data\n",
    "        electricity['hour'] = electricity['datetime'].dt.hour\n",
    "        #Merging train data with client data\n",
    "        data= data.merge(client.drop(columns = ['date']), how='left', on=['data_block_id', 'county', 'is_business', 'product_type'])\n",
    "\n",
    "        data= data.merge(gas[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n",
    "\n",
    "        data= data.merge(electricity[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour', 'data_block_id'])\n",
    "\n",
    "        data= data.merge(forecast_weather_datetime, how='left', on=['datetime'])\n",
    "\n",
    "        data= data.merge(forecast_weather_datetime_county, how='left', on=['datetime', 'county'],\n",
    "                         suffixes= ('_fcast_mean','_fcast_mean_by_county'))\n",
    "\n",
    "        hist_weather_datetime['hour']= hist_weather_datetime['datetime'].dt.hour\n",
    "        hist_weather_datetime_county['hour']= hist_weather_datetime_county['datetime'].dt.hour\n",
    "\n",
    "        hist_weather_datetime.drop_duplicates(inplace=True)\n",
    "        hist_weather_datetime_county.drop_duplicates(inplace=True)\n",
    "        hist_weather_datetime.drop('datetime', axis= 1, inplace= True)\n",
    "        hist_weather_datetime_county.drop('datetime', axis= 1, inplace= True)\n",
    "\n",
    "        #Merging hist_weather_datetime with train data\n",
    "        data= data.merge(hist_weather_datetime, how='left', on=['data_block_id', 'hour'])\n",
    "\n",
    "        data= data.merge(hist_weather_datetime_county, how='left', on=['data_block_id', 'county', 'hour'],suffixes= ('_hist_mean','_hist_mean_by_county'))\n",
    "\n",
    "        data= data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n",
    "\n",
    "        #Dropping uneeded data\n",
    "        data.drop(['level_0', 'level_1', 'row_id', 'data_block_id'], axis= 1, inplace= True)\n",
    "        return data\n",
    "\n",
    "    def create_revealed_targets_train(self, data, N_day_lags):\n",
    "\n",
    "        original_datetime = data['datetime']\n",
    "        revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n",
    "\n",
    "        #Create revealed targets for n days lags\n",
    "        for day_lag in range(2, N_day_lags+1):\n",
    "            revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n",
    "            data = data.merge(revealed_targets, \n",
    "                              how='left', \n",
    "                              on = ['datetime', 'prediction_unit_id', 'is_consumption'],\n",
    "                              suffixes = ('', f'_{day_lag}_days_ago')\n",
    "                             )\n",
    "        data['datetime'] = data['datetime'].astype('int64')\n",
    "        return data\n",
    "\n",
    "    def get_agg_target_lag(self, df):\n",
    "\n",
    "        tgt_lag_columns = [c for c in df.columns if '_days_ago' in c]\n",
    "\n",
    "        for m in ['mean', 'std', 'var', 'median', 'max', 'min']:\n",
    "            df[f'target_{m}'] = df[tgt_lag_columns].agg(m, axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee7501",
   "metadata": {
    "papermill": {
     "duration": 0.006412,
     "end_time": "2024-01-05T08:04:33.888494",
     "exception": false,
     "start_time": "2024-01-05T08:04:33.882082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcef42f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:04:33.906367Z",
     "iopub.status.busy": "2024-01-05T08:04:33.904995Z",
     "iopub.status.idle": "2024-01-05T08:04:33.913061Z",
     "shell.execute_reply": "2024-01-05T08:04:33.912245Z"
    },
    "papermill": {
     "duration": 0.020456,
     "end_time": "2024-01-05T08:04:33.915550",
     "exception": false,
     "start_time": "2024-01-05T08:04:33.895094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_logs(df):\n",
    "    #Log columns with outliers\n",
    "    to_log= ['installed_capacity', 'euros_per_mwh', 'temperature_fcast_mean', 'dewpoint_fcast_mean',\n",
    "            'cloudcover_high_fcast_mean', 'cloudcover_low_fcast_mean', 'cloudcover_mid_fcast_mean', 'cloudcover_total_fcast_mean',\n",
    "            '10_metre_u_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean', 'direct_solar_radiation_fcast_mean',\n",
    "            'snowfall_fcast_mean', 'total_precipitation_fcast_mean', 'temperature_fcast_mean_by_county', 'dewpoint_fcast_mean_by_county',\n",
    "            'cloudcover_high_fcast_mean_by_county', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_mid_fcast_mean_by_county',\n",
    "            'cloudcover_total_fcast_mean_by_county', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean_by_county',\n",
    "            'surface_solar_radiation_downwards_fcast_mean_by_county', 'snowfall_fcast_mean_by_county', 'total_precipitation_fcast_mean_by_county',\n",
    "            'rain_hist_mean', 'snowfall_hist_mean', 'windspeed_10m_hist_mean_by_county', 'target_2_days_ago', 'target_3_days_ago',\n",
    "            'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_mean', 'target_std']\n",
    "    for i in to_log:\n",
    "        df[f\"log_{i}\"]= np.where((df[i])!= 0, np.log(df[i]),0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c644bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:04:33.930926Z",
     "iopub.status.busy": "2024-01-05T08:04:33.930068Z",
     "iopub.status.idle": "2024-01-05T08:09:21.397916Z",
     "shell.execute_reply": "2024-01-05T08:09:21.396415Z"
    },
    "papermill": {
     "duration": 287.485693,
     "end_time": "2024-01-05T08:09:21.407954",
     "exception": false,
     "start_time": "2024-01-05T08:04:33.922261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the number of columns = 121 \n",
      " ['10_metre_u_wind_component_fcast_mean', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean_by_county', 'cloudcover_high_fcast_mean', 'cloudcover_high_fcast_mean_by_county', 'cloudcover_high_hist_mean', 'cloudcover_high_hist_mean_by_county', 'cloudcover_low_fcast_mean', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_low_hist_mean', 'cloudcover_low_hist_mean_by_county', 'cloudcover_mid_fcast_mean', 'cloudcover_mid_fcast_mean_by_county', 'cloudcover_mid_hist_mean', 'cloudcover_mid_hist_mean_by_county', 'cloudcover_total_fcast_mean', 'cloudcover_total_fcast_mean_by_county', 'cloudcover_total_hist_mean', 'cloudcover_total_hist_mean_by_county', 'cos_dayofyear', 'cos_hour', 'county', 'datetime', 'day', 'dayofweek', 'dayofyear', 'dewpoint_fcast_mean', 'dewpoint_fcast_mean_by_county', 'dewpoint_hist_mean', 'dewpoint_hist_mean_by_county', 'diffuse_radiation_hist_mean', 'diffuse_radiation_hist_mean_by_county', 'direct_solar_radiation_fcast_mean', 'direct_solar_radiation_fcast_mean_by_county', 'direct_solar_radiation_hist_mean', 'direct_solar_radiation_hist_mean_by_county', 'eic_count', 'euros_per_mwh', 'highest_price_per_mwh', 'hour', 'installed_capacity', 'is_business', 'is_consumption', 'log_10_metre_u_wind_component_fcast_mean', 'log_10_metre_u_wind_component_fcast_mean_by_county', 'log_10_metre_v_wind_component_fcast_mean', 'log_10_metre_v_wind_component_fcast_mean_by_county', 'log_cloudcover_high_fcast_mean', 'log_cloudcover_high_fcast_mean_by_county', 'log_cloudcover_low_fcast_mean', 'log_cloudcover_low_fcast_mean_by_county', 'log_cloudcover_mid_fcast_mean', 'log_cloudcover_mid_fcast_mean_by_county', 'log_cloudcover_total_fcast_mean', 'log_cloudcover_total_fcast_mean_by_county', 'log_dewpoint_fcast_mean', 'log_dewpoint_fcast_mean_by_county', 'log_direct_solar_radiation_fcast_mean', 'log_euros_per_mwh', 'log_installed_capacity', 'log_rain_hist_mean', 'log_snowfall_fcast_mean', 'log_snowfall_fcast_mean_by_county', 'log_snowfall_hist_mean', 'log_surface_solar_radiation_downwards_fcast_mean_by_county', 'log_target_2_days_ago', 'log_target_3_days_ago', 'log_target_4_days_ago', 'log_target_5_days_ago', 'log_target_6_days_ago', 'log_target_7_days_ago', 'log_target_mean', 'log_target_std', 'log_temperature_fcast_mean', 'log_temperature_fcast_mean_by_county', 'log_total_precipitation_fcast_mean', 'log_total_precipitation_fcast_mean_by_county', 'log_windspeed_10m_hist_mean_by_county', 'lowest_price_per_mwh', 'month', 'prediction_unit_id', 'product_type', 'rain_hist_mean', 'rain_hist_mean_by_county', 'shortwave_radiation_hist_mean', 'shortwave_radiation_hist_mean_by_county', 'sin_dayofyear', 'sin_hour', 'snowfall_fcast_mean', 'snowfall_fcast_mean_by_county', 'snowfall_hist_mean', 'snowfall_hist_mean_by_county', 'surface_pressure_hist_mean', 'surface_pressure_hist_mean_by_county', 'surface_solar_radiation_downwards_fcast_mean', 'surface_solar_radiation_downwards_fcast_mean_by_county', 'target', 'target_2_days_ago', 'target_3_days_ago', 'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_max', 'target_mean', 'target_median', 'target_min', 'target_std', 'target_var', 'temperature_fcast_mean', 'temperature_fcast_mean_by_county', 'temperature_hist_mean', 'temperature_hist_mean_by_county', 'total_precipitation_fcast_mean', 'total_precipitation_fcast_mean_by_county', 'winddirection_10m_hist_mean', 'winddirection_10m_hist_mean_by_county', 'windspeed_10m_hist_mean', 'windspeed_10m_hist_mean_by_county', 'year']\n"
     ]
    }
   ],
   "source": [
    "LOADED = False\n",
    "#Specify how many days to lag and applying the function\n",
    "N_day_lags = 7\n",
    "\n",
    "#Data transformation\n",
    "dp = DataProcessing()\n",
    "#Applying the Train function and storing our train data in the (train) variable\n",
    "train = dp.feat_eng_train(train, client_df, hw_df, fw_df, electricity_df, gas_df, locations)\n",
    "train = dp.create_revealed_targets_train(train, N_day_lags)\n",
    "train['sin_hour']= (np.pi * np.sin(train['hour']) / 12)\n",
    "train['cos_hour']= (np.pi * np.cos(train['hour']) / 12)\n",
    "train['sin_dayofyear']= (np.pi * np.sin(train['dayofyear']) / 183)\n",
    "train['cos_dayofyear']= (np.pi * np.cos(train['dayofyear']) / 183)\n",
    "train = dp.get_agg_target_lag(train)\n",
    "#Log columns with outliers\n",
    "train = make_logs(train)\n",
    "# Filter out the year >= 2022\n",
    "train=train[train.year >= 2022]\n",
    "train.to_csv(\"train.csv\")\n",
    "    \n",
    "    \n",
    "print(f\"Training the number of columns = {len(train.columns.tolist())} \\n {sorted(train.columns.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e3873",
   "metadata": {
    "papermill": {
     "duration": 0.006305,
     "end_time": "2024-01-05T08:09:21.420987",
     "exception": false,
     "start_time": "2024-01-05T08:09:21.414682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0219a07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:09:21.436604Z",
     "iopub.status.busy": "2024-01-05T08:09:21.436190Z",
     "iopub.status.idle": "2024-01-05T08:09:24.738755Z",
     "shell.execute_reply": "2024-01-05T08:09:24.737362Z"
    },
    "papermill": {
     "duration": 3.314215,
     "end_time": "2024-01-05T08:09:24.741992",
     "exception": false,
     "start_time": "2024-01-05T08:09:21.427777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Storing training features into numpy arrays\n",
    "X= train[train['is_consumption'] != 0].drop('target', axis= 1).values\n",
    "y= train[train['is_consumption'] != 0]['target']\n",
    "\n",
    "#Storing production targets into an array itself | Will seperate it into another model\n",
    "X2= train[train['is_consumption'] == 0].drop('target', axis= 1).values\n",
    "y2= train[train['is_consumption'] == 0]['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3fad0",
   "metadata": {
    "papermill": {
     "duration": 0.00731,
     "end_time": "2024-01-05T08:09:24.756331",
     "exception": false,
     "start_time": "2024-01-05T08:09:24.749021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3248797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:09:24.771957Z",
     "iopub.status.busy": "2024-01-05T08:09:24.771551Z",
     "iopub.status.idle": "2024-01-05T08:09:27.856269Z",
     "shell.execute_reply": "2024-01-05T08:09:27.855162Z"
    },
    "papermill": {
     "duration": 3.096088,
     "end_time": "2024-01-05T08:09:27.859058",
     "exception": false,
     "start_time": "2024-01-05T08:09:24.762970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOADED = True # False: create and train the model\n",
    "               # True: load the trained model\n",
    "# Split your data\n",
    "SEED = 73\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, y, test_size=0.25, random_state=SEED, shuffle=True)\n",
    "Xtrain2, Xval2, Ytrain2, Yval2 = train_test_split(X2, y2, test_size=0.25, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba037d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:09:27.875121Z",
     "iopub.status.busy": "2024-01-05T08:09:27.874696Z",
     "iopub.status.idle": "2024-01-05T08:09:27.885612Z",
     "shell.execute_reply": "2024-01-05T08:09:27.884362Z"
    },
    "papermill": {
     "duration": 0.022165,
     "end_time": "2024-01-05T08:09:27.888158",
     "exception": false,
     "start_time": "2024-01-05T08:09:27.865993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "common_params = {\n",
    "    'n_iter':4000,'verbose': -1,'random_state':SEED,'objective':'tweedie',\n",
    "    'device':DEVICE, 'n_jobs':4\n",
    "}\n",
    "m_model_params = [\n",
    "    {**common_params,'learning_rate': 0.004811751415536496, 'colsample_bytree': 0.8841841689852410, 'colsample_bynode': 0.4305836942635745, 'reg_alpha': 3.11984157361821, 'reg_lambda': 1.088469732297296, 'min_data_in_leaf': 162, 'max_depth': 16, 'num_leaves': 435},\n",
    "    {**common_params,'learning_rate': 0.007922862526647507, 'colsample_bytree': 0.9052952790963521, 'colsample_bynode': 0.4416947152746856, 'reg_alpha': 3.31471952672932, 'reg_lambda': 1.349570843308307, 'min_data_in_leaf': 185, 'max_depth': 18, 'num_leaves': 445},\n",
    "    {**common_params,'learning_rate': 0.010339736147758608, 'colsample_bytree': 0.9263063801074632, 'colsample_bynode': 0.4527058263857967, 'reg_alpha': 3.62802063709343, 'reg_lambda': 1.650681954419419, 'min_data_in_leaf': 201, 'max_depth': 20, 'num_leaves': 455},\n",
    "    {**common_params,'learning_rate': 0.013090718804096083, 'colsample_bytree': 0.9499770953943448, 'colsample_bynode': 0.4670163857441046, 'reg_aplha': 3.96946065556807, 'reg_lambda': 1.925712107567988, 'min_data_in_leaf': 223, 'max_depth': 22, 'num_leaves': 465},\n",
    "    {**common_params,'learning_rate': 0.015559490612977255, 'colsample_bytree': 0.9682791614810814, 'colsample_bynode': 0.4722023075509447, 'reg_aplha': 4.15624585398345, 'reg_lambda': 2.265053303366992, 'min_data_in_leaf': 254, 'max_depth': 24, 'num_leaves': 475},\n",
    "    {**common_params,'learning_rate': 0.018908744594789185, 'colsample_bytree': 0.9864875442500248, 'colsample_bynode': 0.4832525869590394, 'reg_aplha': 4.35845913192557, 'reg_lambda': 2.355521088983217, 'min_data_in_leaf': 289, 'max_depth': 26, 'num_leaves': 485},\n",
    "    {**common_params,'learning_rate': 0.021819855605890296, 'colsample_bytree': 0.9995986553611359, 'colsample_bynode': 0.4953634970601405, 'reg_alpha': 4.58956024201648, 'reg_lambda': 2.616432197094328, 'min_data_in_leaf': 309, 'max_depth': 28, 'num_leaves': 495}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39193a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:09:27.903324Z",
     "iopub.status.busy": "2024-01-05T08:09:27.902895Z",
     "iopub.status.idle": "2024-01-05T08:09:57.400509Z",
     "shell.execute_reply": "2024-01-05T08:09:57.397555Z"
    },
    "papermill": {
     "duration": 29.51461,
     "end_time": "2024-01-05T08:09:57.409546",
     "exception": false,
     "start_time": "2024-01-05T08:09:27.894936",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LGB model 0 was Loaded!\n",
      "Trained LGB model 1 was Loaded!\n",
      "Trained LGB model 2 was Loaded!\n",
      "Trained LGB model 3 was Loaded!\n",
      "Trained LGB model 4 was Loaded!\n",
      "Trained LGB model 5 was Loaded!\n",
      "Trained LGB model 6 was Loaded!\n"
     ]
    }
   ],
   "source": [
    "## 7 lgbm models\n",
    "m_models = []\n",
    "for i, model_params in enumerate(m_model_params): # Train the m_models\n",
    "    model_file = f'/kaggle/input/energe-models/lgbm_models/m_model{i}.txt'\n",
    "    if LOADED:\n",
    "        lgbm_model = lgb.Booster(model_file=model_file)\n",
    "        print(f'Trained LGB model {i} was Loaded!')\n",
    "    else:\n",
    "        lgbm_model = LGBMRegressor(**model_params)\n",
    "        print('_______________________________________________________')\n",
    "        print('Start')\n",
    "        lgbm_model.fit(Xtrain, Ytrain, eval_set=[(Xval, Yval)], callbacks=[\n",
    "                lgb.callback.early_stopping(stopping_rounds=100),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ])\n",
    "         # Save the model\n",
    "        lgbm_model.booster_.save_model(model_file)\n",
    "        print(f'Trained LGB model {i} was saved!')\n",
    "    m_models.append(lgbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2116d900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:09:57.459655Z",
     "iopub.status.busy": "2024-01-05T08:09:57.456893Z",
     "iopub.status.idle": "2024-01-05T08:09:57.956875Z",
     "shell.execute_reply": "2024-01-05T08:09:57.955503Z"
    },
    "papermill": {
     "duration": 0.528848,
     "end_time": "2024-01-05T08:09:57.959764",
     "exception": false,
     "start_time": "2024-01-05T08:09:57.430916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7054b51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:09:57.978488Z",
     "iopub.status.busy": "2024-01-05T08:09:57.978042Z",
     "iopub.status.idle": "2024-01-05T08:09:57.989738Z",
     "shell.execute_reply": "2024-01-05T08:09:57.988401Z"
    },
    "papermill": {
     "duration": 0.024095,
     "end_time": "2024-01-05T08:09:57.993256",
     "exception": false,
     "start_time": "2024-01-05T08:09:57.969161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_iter': 4000, 'verbose': -1, 'random_state': 73, 'objective': 'tweedie', 'device': 'cpu', 'n_jobs': 4, 'learning_rate': 0.006311751415536496, 'colsample_bytree': 0.844184168985241, 'colsample_bynode': 0.4305836942635745, 'lambda_l1': 3.21984157361821, 'lambda_l2': 1.108469732297296, 'min_data_in_leaf': 53, 'max_depth': 10, 'min_data_per_groups': 39, 'num_leaves': 435}\n"
     ]
    }
   ],
   "source": [
    "## another 7 lgbm models\n",
    "common_params = {\n",
    "    'n_iter':4000,'verbose': -1,'random_state':SEED,'objective':'tweedie',\n",
    "    'device':DEVICE, 'n_jobs':4\n",
    "}\n",
    "n_model_params =[\n",
    "    {**common_params,'learning_rate': 0.006311751415536496, 'colsample_bytree': 0.8441841689852410, 'colsample_bynode': 0.4305836942635745, 'lambda_l1': 3.21984157361821, 'lambda_l2': 1.108469732297296, 'min_data_in_leaf': 53, 'max_depth': 10,  'min_data_per_groups': 39,'num_leaves': 435},\n",
    "    {**common_params,'learning_rate': 0.008228625036647597, 'colsample_bytree': 0.8652952790963521, 'colsample_bynode': 0.4416947152746856, 'lambda_l1': 3.41471952672932, 'lambda_l2': 1.349570843308307, 'min_data_in_leaf': 58, 'max_depth': 11,  'min_data_per_groups': 49,'num_leaves': 445},\n",
    "    {**common_params,'learning_rate': 0.010339736147758608, 'colsample_bytree': 0.8893063801074632, 'colsample_bynode': 0.4527058263857967, 'lambda_l1': 3.62802063709343, 'lambda_l2': 1.650681954419419, 'min_data_in_leaf': 63, 'max_depth': 13,  'min_data_per_groups': 59,'num_leaves': 455},\n",
    "    {**common_params,'learning_rate': 0.012090718804096083, 'colsample_bytree': 0.9099770953943448, 'colsample_bynode': 0.4670163857441046, 'lambda_l1': 3.86946065556807, 'lambda_l2': 1.925712107567988, 'min_data_in_leaf': 68, 'max_depth': 15,  'min_data_per_groups': 69,'num_leaves': 465},\n",
    "    {**common_params,'learning_rate': 0.014559490612977255, 'colsample_bytree': 0.9282791614810814, 'colsample_bynode': 0.4722023075509447, 'lambda_l1': 4.05624585398343, 'lambda_l2': 2.265053303366992, 'min_data_in_leaf': 73, 'max_depth': 17,  'min_data_per_groups': 79,'num_leaves': 475},\n",
    "    {**common_params,'learning_rate': 0.016908744594789185, 'colsample_bytree': 0.9534875442500248, 'colsample_bynode': 0.4832525869590394, 'lambda_l1': 4.25845913192557, 'lambda_l2': 2.555521088983217, 'min_data_in_leaf': 78, 'max_depth': 19,  'min_data_per_groups': 89,'num_leaves': 485},\n",
    "    {**common_params,'learning_rate': 0.018819855605890296, 'colsample_bytree': 0.9715986553611359, 'colsample_bynode': 0.4953634970601405, 'lambda_l1': 4.58956024201648, 'lambda_l2': 2.816432197094328, 'min_data_in_leaf': 83, 'max_depth': 21,  'min_data_per_groups': 99,'num_leaves': 495}\n",
    "]\n",
    "print(n_model_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d707e1bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:09:58.010039Z",
     "iopub.status.busy": "2024-01-05T08:09:58.009267Z",
     "iopub.status.idle": "2024-01-05T08:10:18.186316Z",
     "shell.execute_reply": "2024-01-05T08:10:18.184385Z"
    },
    "papermill": {
     "duration": 20.189601,
     "end_time": "2024-01-05T08:10:18.190473",
     "exception": false,
     "start_time": "2024-01-05T08:09:58.000872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LGB model 0 was Loaded!\n",
      "Trained LGB model 1 was Loaded!\n",
      "Trained LGB model 2 was Loaded!\n",
      "Trained LGB model 3 was Loaded!\n",
      "Trained LGB model 4 was Loaded!\n",
      "Trained LGB model 5 was Loaded!\n",
      "Trained LGB model 6 was Loaded!\n"
     ]
    }
   ],
   "source": [
    "n_models = []\n",
    "for i, model_params in enumerate(n_model_params):\n",
    "    model_file = f'/kaggle/input/energe-models/lgbm_models/n_model{i}.txt'\n",
    "    if LOADED:\n",
    "        lgbm_model = lgb.Booster(model_file=model_file)\n",
    "        print(f'Trained LGB model {i} was Loaded!')\n",
    "    else:\n",
    "        lgbm_model = LGBMRegressor(**model_params)\n",
    "        print('_______________________________________________________')\n",
    "        print('Start')\n",
    "        lgbm_model.fit(Xtrain2, Ytrain2, eval_set=[(Xval2, Yval2)], callbacks=[\n",
    "                lgb.callback.early_stopping(stopping_rounds=100),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ])\n",
    "         # Save the model\n",
    "        lgbm_model.booster_.save_model(model_file)\n",
    "        print(f'Trained LGB model {i} was saved!')\n",
    "    n_models.append(lgbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "711019b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:10:18.217765Z",
     "iopub.status.busy": "2024-01-05T08:10:18.216074Z",
     "iopub.status.idle": "2024-01-05T08:10:18.599355Z",
     "shell.execute_reply": "2024-01-05T08:10:18.598014Z"
    },
    "papermill": {
     "duration": 0.39742,
     "end_time": "2024-01-05T08:10:18.602533",
     "exception": false,
     "start_time": "2024-01-05T08:10:18.205113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927a138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T18:26:07.884617Z",
     "iopub.status.busy": "2023-11-19T18:26:07.884215Z",
     "iopub.status.idle": "2023-11-19T18:26:07.889926Z",
     "shell.execute_reply": "2023-11-19T18:26:07.888991Z",
     "shell.execute_reply.started": "2023-11-19T18:26:07.884584Z"
    },
    "papermill": {
     "duration": 0.009681,
     "end_time": "2024-01-05T08:10:18.625144",
     "exception": false,
     "start_time": "2024-01-05T08:10:18.615463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Test function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05fa0f24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:10:18.644663Z",
     "iopub.status.busy": "2024-01-05T08:10:18.644097Z",
     "iopub.status.idle": "2024-01-05T08:10:18.679587Z",
     "shell.execute_reply": "2024-01-05T08:10:18.677760Z"
    },
    "papermill": {
     "duration": 0.049828,
     "end_time": "2024-01-05T08:10:18.683132",
     "exception": false,
     "start_time": "2024-01-05T08:10:18.633304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataProcessing():\n",
    "\n",
    "    def feat_eng_test(self, data, client, hist_weather, forecast_weather, electricity, gas, locations):\n",
    "\n",
    "        data= data.rename(columns={'prediction_datetime' : 'datetime'})\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'], utc=True)\n",
    "        electricity = electricity.rename(columns= {'forecast_date' : 'datetime'})\n",
    "        electricity['datetime'] = pd.to_datetime(electricity['datetime'], utc= True)\n",
    "        locations = locations.drop('Unnamed: 0', axis= 1) \n",
    "        forecast_weather[['latitude', 'longitude']] = forecast_weather[['latitude', 'longitude']].astype(float).round(1)\n",
    "        forecast_weather= forecast_weather.merge(locations, how='left', on=['longitude','latitude'])\n",
    "        forecast_weather.dropna(axis= 0, inplace= True)    \n",
    "        forecast_weather['county'] = forecast_weather['county'].astype('int64')\n",
    "        forecast_weather.drop(['origin_datetime', 'latitude', 'longitude', 'hours_ahead', 'data_block_id'], axis=1, inplace= True)\n",
    "        forecast_weather.rename(columns={'forecast_datetime': 'datetime'}, inplace= True)\n",
    "        forecast_weather['datetime']= pd.to_datetime(forecast_weather['datetime'], utc= True)\n",
    "        forecast_weather_datetime= forecast_weather.groupby([forecast_weather['datetime'].\n",
    "                                                dt.to_period('h')])[list(forecast_weather.drop(['county',\n",
    "                                                                                                'datetime'], axis= 1)\n",
    "                                                                         .columns)].mean().reset_index()\n",
    "        forecast_weather_datetime['datetime']= pd.to_datetime(\n",
    "            forecast_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n",
    "\n",
    "        forecast_weather_datetime_county= forecast_weather.groupby(['county',forecast_weather['datetime'].\n",
    "                                  dt.to_period('h')])[list(forecast_weather.drop(['county',\n",
    "                                                                                  'datetime'], axis= 1)\n",
    "                                                           .columns)].mean().reset_index()\n",
    "        forecast_weather_datetime_county['datetime']= pd.to_datetime(\n",
    "            forecast_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "\n",
    "\n",
    "        hist_weather[['latitude', 'longitude']] = hist_weather[['latitude', 'longitude']].astype(float).round(1)\n",
    "\n",
    "        hist_weather= hist_weather.merge(locations, how='left', on=['longitude','latitude'])    \n",
    "\n",
    "        hist_weather.dropna(axis= 0, inplace= True)\n",
    "\n",
    "        hist_weather.drop(['latitude', 'longitude'], axis=1, inplace= True)\n",
    "\n",
    "        hist_weather['county'] = hist_weather['county'].astype('int64')\n",
    "\n",
    "        hist_weather['datetime']= pd.to_datetime(hist_weather['datetime'], utc= True)\n",
    "\n",
    "\n",
    "        hist_weather_datetime= hist_weather.groupby([hist_weather['datetime'].dt.to_period('h')])\n",
    "        \n",
    "        hist_weather_datetime = hist_weather_datetime[list(hist_weather.drop(['county',\n",
    "                                                                              'datetime',\n",
    "                                                                              'data_block_id'], axis= 1).columns)\n",
    "                                                                             ].mean().reset_index()    \n",
    "\n",
    "        hist_weather_datetime['datetime']= pd.to_datetime(hist_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n",
    "        hist_weather_datetime= hist_weather_datetime.merge(hist_weather[['datetime', 'data_block_id']], \n",
    "                                                           how='left', on='datetime')\n",
    "\n",
    "\n",
    "        hist_weather_datetime_county = hist_weather.groupby(['county',\n",
    "                                                            hist_weather['datetime'].dt.to_period('h')])\n",
    "        hist_weather_datetime_county = hist_weather_datetime_county[list(hist_weather.drop(['county',\n",
    "                                                                                            'datetime',\n",
    "                                                                                            'data_block_id'], axis= 1).columns)].mean().reset_index() \n",
    "        hist_weather_datetime_county['datetime']= pd.to_datetime(hist_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "        hist_weather_datetime_county= hist_weather_datetime_county.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "\n",
    "        data['year'] = data['datetime'].dt.year\n",
    "        data['month'] = data['datetime'].dt.month\n",
    "        data['day'] = data['datetime'].dt.day\n",
    "        data['hour'] = data['datetime'].dt.hour\n",
    "        data['dayofweek']= data['datetime'].dt.dayofweek\n",
    "        data['dayofyear']= data['datetime'].dt.dayofyear\n",
    "\n",
    "        electricity['hour'] = electricity['datetime'].dt.hour\n",
    "\n",
    "        data= data.merge(client.drop(columns = ['date']), how='left', on=['data_block_id', 'county', 'is_business', 'product_type'])\n",
    "        data= data.merge(gas[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n",
    "        data= data.merge(electricity[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour', 'data_block_id'])\n",
    "        data= data.merge(forecast_weather_datetime, how='left', on=['datetime'])\n",
    "        data= data.merge(forecast_weather_datetime_county, how='left', on=['datetime', 'county'],\n",
    "                         suffixes= ('_fcast_mean','_fcast_mean_by_county'))\n",
    "\n",
    "        hist_weather_datetime['hour']= hist_weather_datetime['datetime'].dt.hour\n",
    "        hist_weather_datetime_county['hour']= hist_weather_datetime_county['datetime'].dt.hour\n",
    "\n",
    "        hist_weather_datetime.drop_duplicates(inplace=True)\n",
    "        hist_weather_datetime_county.drop_duplicates(inplace=True)\n",
    "        hist_weather_datetime.drop('datetime', axis= 1, inplace= True)\n",
    "        hist_weather_datetime_county.drop('datetime', axis= 1, inplace= True)\n",
    "\n",
    "\n",
    "        data= data.merge(hist_weather_datetime, how='left', on=['data_block_id', 'hour'])\n",
    "\n",
    "        data= data.merge(hist_weather_datetime_county, how='left', on=['data_block_id', 'county', 'hour'],\n",
    "                         suffixes= ('_hist_mean','_hist_mean_by_county'))\n",
    "\n",
    "        data= data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n",
    "\n",
    "        data.drop(['level_0', 'level_1', 'row_id', 'data_block_id'], axis= 1, inplace= True)\n",
    "\n",
    "        return data\n",
    "    def create_revealed_targets_test(self, data, previous_revealed_targets, N_day_lags):\n",
    "        #   Create new test data based on previous_revealed_targets and N_day_lags \n",
    "        for count, revealed_targets in enumerate(previous_revealed_targets) :\n",
    "            day_lag = count + 2\n",
    "            # Get hour\n",
    "            revealed_targets['hour'] = pd.to_datetime(revealed_targets['datetime'], utc= True).dt.hour\n",
    "            # Select columns and rename target\n",
    "            revealed_targets = revealed_targets[['hour', 'prediction_unit_id', 'is_consumption', 'target']]\n",
    "            revealed_targets = revealed_targets.rename(columns = {\"target\" : f\"target_{day_lag}_days_ago\"})\n",
    "            # Add past revealed targets\n",
    "            data = pd.merge(data,\n",
    "                            revealed_targets,\n",
    "                            how = 'left',\n",
    "                            on = ['hour', 'prediction_unit_id', 'is_consumption'],\n",
    "                           )\n",
    "\n",
    "        # If revealed_target_columns not available, replace by nan\n",
    "        all_revealed_columns = [f\"target_{day_lag}_days_ago\" for day_lag in range(2, N_day_lags+1)]\n",
    "        missing_columns = list(set(all_revealed_columns) - set(data.columns))\n",
    "        data[missing_columns] = np.nan \n",
    "        \n",
    "        return data\n",
    "\n",
    "    def get_agg_target_lag(self, df):\n",
    "\n",
    "        tgt_lag_columns = [c for c in df.columns if '_days_ago' in c]\n",
    "\n",
    "        for m in ['mean', 'std', 'var', 'median', 'max', 'min']:\n",
    "            df[f'target_{m}'] = df[tgt_lag_columns].agg(m, axis=1)\n",
    "            \n",
    "        df['target'] = np.nan\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47046ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T19:52:39.007738Z",
     "iopub.status.busy": "2023-12-17T19:52:39.007305Z",
     "iopub.status.idle": "2023-12-17T19:52:39.012084Z",
     "shell.execute_reply": "2023-12-17T19:52:39.011305Z",
     "shell.execute_reply.started": "2023-12-17T19:52:39.007703Z"
    },
    "papermill": {
     "duration": 0.008462,
     "end_time": "2024-01-05T08:10:18.701962",
     "exception": false,
     "start_time": "2024-01-05T08:10:18.693500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68cd3719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:10:18.724325Z",
     "iopub.status.busy": "2024-01-05T08:10:18.723609Z",
     "iopub.status.idle": "2024-01-05T08:10:18.734689Z",
     "shell.execute_reply": "2024-01-05T08:10:18.733515Z"
    },
    "papermill": {
     "duration": 0.025309,
     "end_time": "2024-01-05T08:10:18.738117",
     "exception": false,
     "start_time": "2024-01-05T08:10:18.712808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95264ee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:10:18.760966Z",
     "iopub.status.busy": "2024-01-05T08:10:18.760123Z",
     "iopub.status.idle": "2024-01-05T08:13:22.500759Z",
     "shell.execute_reply": "2024-01-05T08:13:22.499568Z"
    },
    "papermill": {
     "duration": 183.752921,
     "end_time": "2024-01-05T08:13:22.503451",
     "exception": false,
     "start_time": "2024-01-05T08:10:18.750530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "df_test the number of columns = 121\n",
      " ['10_metre_u_wind_component_fcast_mean', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean_by_county', 'cloudcover_high_fcast_mean', 'cloudcover_high_fcast_mean_by_county', 'cloudcover_high_hist_mean', 'cloudcover_high_hist_mean_by_county', 'cloudcover_low_fcast_mean', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_low_hist_mean', 'cloudcover_low_hist_mean_by_county', 'cloudcover_mid_fcast_mean', 'cloudcover_mid_fcast_mean_by_county', 'cloudcover_mid_hist_mean', 'cloudcover_mid_hist_mean_by_county', 'cloudcover_total_fcast_mean', 'cloudcover_total_fcast_mean_by_county', 'cloudcover_total_hist_mean', 'cloudcover_total_hist_mean_by_county', 'cos_dayofyear', 'cos_hour', 'county', 'datetime', 'day', 'dayofweek', 'dayofyear', 'dewpoint_fcast_mean', 'dewpoint_fcast_mean_by_county', 'dewpoint_hist_mean', 'dewpoint_hist_mean_by_county', 'diffuse_radiation_hist_mean', 'diffuse_radiation_hist_mean_by_county', 'direct_solar_radiation_fcast_mean', 'direct_solar_radiation_fcast_mean_by_county', 'direct_solar_radiation_hist_mean', 'direct_solar_radiation_hist_mean_by_county', 'eic_count', 'euros_per_mwh', 'highest_price_per_mwh', 'hour', 'installed_capacity', 'is_business', 'is_consumption', 'log_10_metre_u_wind_component_fcast_mean', 'log_10_metre_u_wind_component_fcast_mean_by_county', 'log_10_metre_v_wind_component_fcast_mean', 'log_10_metre_v_wind_component_fcast_mean_by_county', 'log_cloudcover_high_fcast_mean', 'log_cloudcover_high_fcast_mean_by_county', 'log_cloudcover_low_fcast_mean', 'log_cloudcover_low_fcast_mean_by_county', 'log_cloudcover_mid_fcast_mean', 'log_cloudcover_mid_fcast_mean_by_county', 'log_cloudcover_total_fcast_mean', 'log_cloudcover_total_fcast_mean_by_county', 'log_dewpoint_fcast_mean', 'log_dewpoint_fcast_mean_by_county', 'log_direct_solar_radiation_fcast_mean', 'log_euros_per_mwh', 'log_installed_capacity', 'log_rain_hist_mean', 'log_snowfall_fcast_mean', 'log_snowfall_fcast_mean_by_county', 'log_snowfall_hist_mean', 'log_surface_solar_radiation_downwards_fcast_mean_by_county', 'log_target_2_days_ago', 'log_target_3_days_ago', 'log_target_4_days_ago', 'log_target_5_days_ago', 'log_target_6_days_ago', 'log_target_7_days_ago', 'log_target_mean', 'log_target_std', 'log_temperature_fcast_mean', 'log_temperature_fcast_mean_by_county', 'log_total_precipitation_fcast_mean', 'log_total_precipitation_fcast_mean_by_county', 'log_windspeed_10m_hist_mean_by_county', 'lowest_price_per_mwh', 'month', 'prediction_unit_id', 'product_type', 'rain_hist_mean', 'rain_hist_mean_by_county', 'shortwave_radiation_hist_mean', 'shortwave_radiation_hist_mean_by_county', 'sin_dayofyear', 'sin_hour', 'snowfall_fcast_mean', 'snowfall_fcast_mean_by_county', 'snowfall_hist_mean', 'snowfall_hist_mean_by_county', 'surface_pressure_hist_mean', 'surface_pressure_hist_mean_by_county', 'surface_solar_radiation_downwards_fcast_mean', 'surface_solar_radiation_downwards_fcast_mean_by_county', 'target', 'target_2_days_ago', 'target_3_days_ago', 'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_max', 'target_mean', 'target_median', 'target_min', 'target_std', 'target_var', 'temperature_fcast_mean', 'temperature_fcast_mean_by_county', 'temperature_hist_mean', 'temperature_hist_mean_by_county', 'total_precipitation_fcast_mean', 'total_precipitation_fcast_mean_by_county', 'winddirection_10m_hist_mean', 'winddirection_10m_hist_mean_by_county', 'windspeed_10m_hist_mean', 'windspeed_10m_hist_mean_by_county', 'year'] \n",
      "pred = [ 47.13070337 134.19056085  16.51226835 ...  23.40780759  40.6628074\n",
      "  65.81335486]\n",
      "pred = [ 55.76209851 140.27036306  18.06912875 ...  22.7595201   41.99687438\n",
      "  65.31763632]\n",
      "pred = [ 48.18594776 107.75615103  16.15063991 ...  20.53144066  40.70803833\n",
      "  55.10044365]\n",
      "pred = [ 43.45159535 105.22209307  14.71323055 ...  21.17798754  42.47078029\n",
      "  66.96169567]\n",
      "pred = [ 39.65582012 118.58044363  16.60544446 ...  24.37187032  40.52828625\n",
      "  57.28828135]\n",
      "pred = [46.86722608 99.85098788 16.6751283  ... 21.44050897 46.49613808\n",
      " 60.86545187]\n",
      "pred = [39.60370305 71.14210183 13.29090621 ... 18.31736839 33.23130675\n",
      " 46.58854809]\n",
      "pred = [5.11982125e-02 5.19882763e-01 2.78437389e-04 ... 1.21509966e-01\n",
      " 1.92044707e-03 2.72896960e-01]\n",
      "pred = [4.19782242e-02 4.31822410e-01 1.75164314e-04 ... 1.00059835e-01\n",
      " 1.28602716e-03 2.46314957e-01]\n",
      "pred = [6.00652322e-02 5.43896401e-01 1.55899475e-04 ... 1.19447252e-01\n",
      " 2.44201316e-03 3.51236955e-01]\n",
      "pred = [1.16210288e-01 9.01014500e-01 2.86459340e-04 ... 2.17154360e-01\n",
      " 4.46004605e-03 5.31917280e-01]\n",
      "pred = [1.06307168e-01 7.14724247e-01 3.13771675e-04 ... 2.13203652e-01\n",
      " 3.59441572e-03 4.50646911e-01]\n",
      "pred = [1.07262954e-01 8.60918812e-01 5.99558683e-04 ... 2.60410714e-01\n",
      " 3.46882382e-03 4.54094139e-01]\n",
      "pred = [1.82958078e-01 1.51050676e+00 6.66294149e-04 ... 4.08754786e-01\n",
      " 4.81251021e-03 8.20984274e-01]\n",
      "df_test the number of columns = 121\n",
      " ['10_metre_u_wind_component_fcast_mean', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean_by_county', 'cloudcover_high_fcast_mean', 'cloudcover_high_fcast_mean_by_county', 'cloudcover_high_hist_mean', 'cloudcover_high_hist_mean_by_county', 'cloudcover_low_fcast_mean', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_low_hist_mean', 'cloudcover_low_hist_mean_by_county', 'cloudcover_mid_fcast_mean', 'cloudcover_mid_fcast_mean_by_county', 'cloudcover_mid_hist_mean', 'cloudcover_mid_hist_mean_by_county', 'cloudcover_total_fcast_mean', 'cloudcover_total_fcast_mean_by_county', 'cloudcover_total_hist_mean', 'cloudcover_total_hist_mean_by_county', 'cos_dayofyear', 'cos_hour', 'county', 'datetime', 'day', 'dayofweek', 'dayofyear', 'dewpoint_fcast_mean', 'dewpoint_fcast_mean_by_county', 'dewpoint_hist_mean', 'dewpoint_hist_mean_by_county', 'diffuse_radiation_hist_mean', 'diffuse_radiation_hist_mean_by_county', 'direct_solar_radiation_fcast_mean', 'direct_solar_radiation_fcast_mean_by_county', 'direct_solar_radiation_hist_mean', 'direct_solar_radiation_hist_mean_by_county', 'eic_count', 'euros_per_mwh', 'highest_price_per_mwh', 'hour', 'installed_capacity', 'is_business', 'is_consumption', 'log_10_metre_u_wind_component_fcast_mean', 'log_10_metre_u_wind_component_fcast_mean_by_county', 'log_10_metre_v_wind_component_fcast_mean', 'log_10_metre_v_wind_component_fcast_mean_by_county', 'log_cloudcover_high_fcast_mean', 'log_cloudcover_high_fcast_mean_by_county', 'log_cloudcover_low_fcast_mean', 'log_cloudcover_low_fcast_mean_by_county', 'log_cloudcover_mid_fcast_mean', 'log_cloudcover_mid_fcast_mean_by_county', 'log_cloudcover_total_fcast_mean', 'log_cloudcover_total_fcast_mean_by_county', 'log_dewpoint_fcast_mean', 'log_dewpoint_fcast_mean_by_county', 'log_direct_solar_radiation_fcast_mean', 'log_euros_per_mwh', 'log_installed_capacity', 'log_rain_hist_mean', 'log_snowfall_fcast_mean', 'log_snowfall_fcast_mean_by_county', 'log_snowfall_hist_mean', 'log_surface_solar_radiation_downwards_fcast_mean_by_county', 'log_target_2_days_ago', 'log_target_3_days_ago', 'log_target_4_days_ago', 'log_target_5_days_ago', 'log_target_6_days_ago', 'log_target_7_days_ago', 'log_target_mean', 'log_target_std', 'log_temperature_fcast_mean', 'log_temperature_fcast_mean_by_county', 'log_total_precipitation_fcast_mean', 'log_total_precipitation_fcast_mean_by_county', 'log_windspeed_10m_hist_mean_by_county', 'lowest_price_per_mwh', 'month', 'prediction_unit_id', 'product_type', 'rain_hist_mean', 'rain_hist_mean_by_county', 'shortwave_radiation_hist_mean', 'shortwave_radiation_hist_mean_by_county', 'sin_dayofyear', 'sin_hour', 'snowfall_fcast_mean', 'snowfall_fcast_mean_by_county', 'snowfall_hist_mean', 'snowfall_hist_mean_by_county', 'surface_pressure_hist_mean', 'surface_pressure_hist_mean_by_county', 'surface_solar_radiation_downwards_fcast_mean', 'surface_solar_radiation_downwards_fcast_mean_by_county', 'target', 'target_2_days_ago', 'target_3_days_ago', 'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_max', 'target_mean', 'target_median', 'target_min', 'target_std', 'target_var', 'temperature_fcast_mean', 'temperature_fcast_mean_by_county', 'temperature_hist_mean', 'temperature_hist_mean_by_county', 'total_precipitation_fcast_mean', 'total_precipitation_fcast_mean_by_county', 'winddirection_10m_hist_mean', 'winddirection_10m_hist_mean_by_county', 'windspeed_10m_hist_mean', 'windspeed_10m_hist_mean_by_county', 'year'] \n",
      "pred = [ 44.19430029 166.95188838  13.78550249 ...  25.02759023  34.27707033\n",
      "  66.72931109]\n",
      "pred = [ 44.192084   173.45413698  14.12334473 ...  24.79503461  34.2583613\n",
      "  62.69856759]\n",
      "pred = [ 38.44172416 130.13347052  12.20696331 ...  22.21266251  26.18811619\n",
      "  51.35880133]\n",
      "pred = [ 41.64305634 117.80002252  14.06134749 ...  24.55158677  34.47607376\n",
      "  50.78960509]\n",
      "pred = [ 39.09162971 152.61429913  14.37122564 ...  28.145322    31.04388271\n",
      "  61.1381179 ]\n",
      "pred = [ 33.24270757 136.54379275  10.03808964 ...  17.69045063  26.41401573\n",
      "  50.30075472]\n",
      "pred = [ 31.69213823 120.03129629   8.8066458  ...  21.89116262  24.42945112\n",
      "  46.90572684]\n",
      "pred = [7.21358519e-02 2.05365136e+00 3.93628001e-05 ... 4.10653781e-01\n",
      " 3.51233051e-04 7.60991148e-01]\n",
      "pred = [4.59946055e-02 1.45501775e+00 2.18582037e-05 ... 3.34589832e-01\n",
      " 2.09424102e-04 6.26191772e-01]\n",
      "pred = [5.92777799e-02 1.79780063e+00 3.26248714e-05 ... 3.52721136e-01\n",
      " 2.44474009e-04 7.50657926e-01]\n",
      "pred = [1.46791459e-01 3.05574052e+00 6.00607183e-05 ... 8.46045238e-01\n",
      " 4.87595058e-04 1.39443981e+00]\n",
      "pred = [1.51585642e-01 2.76253053e+00 9.64220914e-05 ... 7.26982441e-01\n",
      " 7.20241479e-04 1.11382934e+00]\n",
      "pred = [1.79622599e-01 5.17255826e+00 1.32256261e-04 ... 1.22687975e+00\n",
      " 5.01584528e-04 1.84463249e+00]\n",
      "pred = [2.43747030e-01 5.92445572e+00 1.31998202e-04 ... 1.00790938e+00\n",
      " 6.73405019e-04 1.60653648e+00]\n",
      "df_test the number of columns = 121\n",
      " ['10_metre_u_wind_component_fcast_mean', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean_by_county', 'cloudcover_high_fcast_mean', 'cloudcover_high_fcast_mean_by_county', 'cloudcover_high_hist_mean', 'cloudcover_high_hist_mean_by_county', 'cloudcover_low_fcast_mean', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_low_hist_mean', 'cloudcover_low_hist_mean_by_county', 'cloudcover_mid_fcast_mean', 'cloudcover_mid_fcast_mean_by_county', 'cloudcover_mid_hist_mean', 'cloudcover_mid_hist_mean_by_county', 'cloudcover_total_fcast_mean', 'cloudcover_total_fcast_mean_by_county', 'cloudcover_total_hist_mean', 'cloudcover_total_hist_mean_by_county', 'cos_dayofyear', 'cos_hour', 'county', 'datetime', 'day', 'dayofweek', 'dayofyear', 'dewpoint_fcast_mean', 'dewpoint_fcast_mean_by_county', 'dewpoint_hist_mean', 'dewpoint_hist_mean_by_county', 'diffuse_radiation_hist_mean', 'diffuse_radiation_hist_mean_by_county', 'direct_solar_radiation_fcast_mean', 'direct_solar_radiation_fcast_mean_by_county', 'direct_solar_radiation_hist_mean', 'direct_solar_radiation_hist_mean_by_county', 'eic_count', 'euros_per_mwh', 'highest_price_per_mwh', 'hour', 'installed_capacity', 'is_business', 'is_consumption', 'log_10_metre_u_wind_component_fcast_mean', 'log_10_metre_u_wind_component_fcast_mean_by_county', 'log_10_metre_v_wind_component_fcast_mean', 'log_10_metre_v_wind_component_fcast_mean_by_county', 'log_cloudcover_high_fcast_mean', 'log_cloudcover_high_fcast_mean_by_county', 'log_cloudcover_low_fcast_mean', 'log_cloudcover_low_fcast_mean_by_county', 'log_cloudcover_mid_fcast_mean', 'log_cloudcover_mid_fcast_mean_by_county', 'log_cloudcover_total_fcast_mean', 'log_cloudcover_total_fcast_mean_by_county', 'log_dewpoint_fcast_mean', 'log_dewpoint_fcast_mean_by_county', 'log_direct_solar_radiation_fcast_mean', 'log_euros_per_mwh', 'log_installed_capacity', 'log_rain_hist_mean', 'log_snowfall_fcast_mean', 'log_snowfall_fcast_mean_by_county', 'log_snowfall_hist_mean', 'log_surface_solar_radiation_downwards_fcast_mean_by_county', 'log_target_2_days_ago', 'log_target_3_days_ago', 'log_target_4_days_ago', 'log_target_5_days_ago', 'log_target_6_days_ago', 'log_target_7_days_ago', 'log_target_mean', 'log_target_std', 'log_temperature_fcast_mean', 'log_temperature_fcast_mean_by_county', 'log_total_precipitation_fcast_mean', 'log_total_precipitation_fcast_mean_by_county', 'log_windspeed_10m_hist_mean_by_county', 'lowest_price_per_mwh', 'month', 'prediction_unit_id', 'product_type', 'rain_hist_mean', 'rain_hist_mean_by_county', 'shortwave_radiation_hist_mean', 'shortwave_radiation_hist_mean_by_county', 'sin_dayofyear', 'sin_hour', 'snowfall_fcast_mean', 'snowfall_fcast_mean_by_county', 'snowfall_hist_mean', 'snowfall_hist_mean_by_county', 'surface_pressure_hist_mean', 'surface_pressure_hist_mean_by_county', 'surface_solar_radiation_downwards_fcast_mean', 'surface_solar_radiation_downwards_fcast_mean_by_county', 'target', 'target_2_days_ago', 'target_3_days_ago', 'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_max', 'target_mean', 'target_median', 'target_min', 'target_std', 'target_var', 'temperature_fcast_mean', 'temperature_fcast_mean_by_county', 'temperature_hist_mean', 'temperature_hist_mean_by_county', 'total_precipitation_fcast_mean', 'total_precipitation_fcast_mean_by_county', 'winddirection_10m_hist_mean', 'winddirection_10m_hist_mean_by_county', 'windspeed_10m_hist_mean', 'windspeed_10m_hist_mean_by_county', 'year'] \n",
      "pred = [ 47.14807104 189.05610864  13.09024697 ...  22.25301806  27.98935709\n",
      "  64.83787594]\n",
      "pred = [ 48.47359975 184.63967761  13.26358615 ...  23.1804673   27.53135286\n",
      "  63.7626505 ]\n",
      "pred = [ 38.89420945 134.78145183  11.94191564 ...  21.02722307  24.50496476\n",
      "  53.4026925 ]\n",
      "pred = [ 41.5883946  134.70018378  12.59682584 ...  20.65566014  26.5557373\n",
      "  52.070066  ]\n",
      "pred = [ 35.95592311 146.48690446  11.62524641 ...  23.46962735  24.43561854\n",
      "  58.77598971]\n",
      "pred = [ 29.57383974 120.89204943   7.64589924 ...  16.79852958  20.51067357\n",
      "  52.49158165]\n",
      "pred = [ 31.70522171 125.62460496   7.86920666 ...  18.8244035   18.48068207\n",
      "  49.9935711 ]\n",
      "pred = [1.19324658e-01 2.17936717e+00 4.13736115e-05 ... 5.91762017e-01\n",
      " 2.62498580e-04 1.83526158e+00]\n",
      "pred = [9.86359070e-02 1.34195904e+00 2.82296542e-05 ... 4.56476794e-01\n",
      " 2.63608823e-04 1.29183244e+00]\n",
      "pred = [1.13301589e-01 1.71365524e+00 3.07154077e-05 ... 4.44075872e-01\n",
      " 2.63679622e-04 1.34894659e+00]\n",
      "pred = [2.31741975e-01 2.42475198e+00 6.72905666e-05 ... 7.99268802e-01\n",
      " 4.58783001e-04 1.91441006e+00]\n",
      "pred = [2.42396084e-01 2.41706026e+00 1.06010579e-04 ... 6.59592828e-01\n",
      " 5.74713268e-04 1.51771022e+00]\n",
      "pred = [2.41634240e-01 3.96635294e+00 1.51568125e-04 ... 1.26142558e+00\n",
      " 4.76660142e-04 2.37077916e+00]\n",
      "pred = [2.65572255e-01 4.23377466e+00 1.07951621e-04 ... 1.05115185e+00\n",
      " 6.97851270e-04 2.05800994e+00]\n",
      "df_test the number of columns = 121\n",
      " ['10_metre_u_wind_component_fcast_mean', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean_by_county', 'cloudcover_high_fcast_mean', 'cloudcover_high_fcast_mean_by_county', 'cloudcover_high_hist_mean', 'cloudcover_high_hist_mean_by_county', 'cloudcover_low_fcast_mean', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_low_hist_mean', 'cloudcover_low_hist_mean_by_county', 'cloudcover_mid_fcast_mean', 'cloudcover_mid_fcast_mean_by_county', 'cloudcover_mid_hist_mean', 'cloudcover_mid_hist_mean_by_county', 'cloudcover_total_fcast_mean', 'cloudcover_total_fcast_mean_by_county', 'cloudcover_total_hist_mean', 'cloudcover_total_hist_mean_by_county', 'cos_dayofyear', 'cos_hour', 'county', 'datetime', 'day', 'dayofweek', 'dayofyear', 'dewpoint_fcast_mean', 'dewpoint_fcast_mean_by_county', 'dewpoint_hist_mean', 'dewpoint_hist_mean_by_county', 'diffuse_radiation_hist_mean', 'diffuse_radiation_hist_mean_by_county', 'direct_solar_radiation_fcast_mean', 'direct_solar_radiation_fcast_mean_by_county', 'direct_solar_radiation_hist_mean', 'direct_solar_radiation_hist_mean_by_county', 'eic_count', 'euros_per_mwh', 'highest_price_per_mwh', 'hour', 'installed_capacity', 'is_business', 'is_consumption', 'log_10_metre_u_wind_component_fcast_mean', 'log_10_metre_u_wind_component_fcast_mean_by_county', 'log_10_metre_v_wind_component_fcast_mean', 'log_10_metre_v_wind_component_fcast_mean_by_county', 'log_cloudcover_high_fcast_mean', 'log_cloudcover_high_fcast_mean_by_county', 'log_cloudcover_low_fcast_mean', 'log_cloudcover_low_fcast_mean_by_county', 'log_cloudcover_mid_fcast_mean', 'log_cloudcover_mid_fcast_mean_by_county', 'log_cloudcover_total_fcast_mean', 'log_cloudcover_total_fcast_mean_by_county', 'log_dewpoint_fcast_mean', 'log_dewpoint_fcast_mean_by_county', 'log_direct_solar_radiation_fcast_mean', 'log_euros_per_mwh', 'log_installed_capacity', 'log_rain_hist_mean', 'log_snowfall_fcast_mean', 'log_snowfall_fcast_mean_by_county', 'log_snowfall_hist_mean', 'log_surface_solar_radiation_downwards_fcast_mean_by_county', 'log_target_2_days_ago', 'log_target_3_days_ago', 'log_target_4_days_ago', 'log_target_5_days_ago', 'log_target_6_days_ago', 'log_target_7_days_ago', 'log_target_mean', 'log_target_std', 'log_temperature_fcast_mean', 'log_temperature_fcast_mean_by_county', 'log_total_precipitation_fcast_mean', 'log_total_precipitation_fcast_mean_by_county', 'log_windspeed_10m_hist_mean_by_county', 'lowest_price_per_mwh', 'month', 'prediction_unit_id', 'product_type', 'rain_hist_mean', 'rain_hist_mean_by_county', 'shortwave_radiation_hist_mean', 'shortwave_radiation_hist_mean_by_county', 'sin_dayofyear', 'sin_hour', 'snowfall_fcast_mean', 'snowfall_fcast_mean_by_county', 'snowfall_hist_mean', 'snowfall_hist_mean_by_county', 'surface_pressure_hist_mean', 'surface_pressure_hist_mean_by_county', 'surface_solar_radiation_downwards_fcast_mean', 'surface_solar_radiation_downwards_fcast_mean_by_county', 'target', 'target_2_days_ago', 'target_3_days_ago', 'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_max', 'target_mean', 'target_median', 'target_min', 'target_std', 'target_var', 'temperature_fcast_mean', 'temperature_fcast_mean_by_county', 'temperature_hist_mean', 'temperature_hist_mean_by_county', 'total_precipitation_fcast_mean', 'total_precipitation_fcast_mean_by_county', 'winddirection_10m_hist_mean', 'winddirection_10m_hist_mean_by_county', 'windspeed_10m_hist_mean', 'windspeed_10m_hist_mean_by_county', 'year'] \n",
      "pred = [ 39.86472309 170.96059013  11.27798178 ...  20.53914788  26.1568532\n",
      "  58.74172209]\n",
      "pred = [ 40.8630745  176.02171009  11.61365947 ...  21.40569114  27.50412991\n",
      "  56.43252803]\n",
      "pred = [ 34.6683199  130.67821399   9.98531448 ...  18.75013485  21.35584748\n",
      "  46.34143183]\n",
      "pred = [ 34.81406577 123.22688473  11.43594992 ...  20.61514488  27.50282471\n",
      "  51.91279251]\n",
      "pred = [ 30.28024047 134.87968636  10.3967314  ...  22.88992968  24.31266696\n",
      "  56.91210032]\n",
      "pred = [ 23.9208151  119.79924645   6.83019731 ...  17.55494191  20.58331438\n",
      "  49.92005419]\n",
      "pred = [ 27.63203657 121.86322381   7.00480151 ...  19.74623794  20.19823695\n",
      "  51.25555642]\n",
      "pred = [1.46969723e-01 3.29826339e+00 4.94556800e-05 ... 5.79608461e-01\n",
      " 8.86006258e-04 1.52356882e+00]\n",
      "pred = [1.45072743e-01 2.38288764e+00 3.79813392e-05 ... 4.81248159e-01\n",
      " 4.86462587e-04 1.06275781e+00]\n",
      "pred = [1.52648804e-01 2.53707314e+00 2.65699379e-05 ... 4.91305003e-01\n",
      " 8.09742583e-04 1.31264943e+00]\n",
      "pred = [2.96687564e-01 4.23474869e+00 8.65831390e-05 ... 9.19895460e-01\n",
      " 1.46375946e-03 1.63287749e+00]\n",
      "pred = [2.63939687e-01 3.92130864e+00 1.44391644e-04 ... 7.49670950e-01\n",
      " 1.89993177e-03 1.42470875e+00]\n",
      "pred = [2.54344510e-01 5.36698481e+00 1.82342873e-04 ... 1.26228696e+00\n",
      " 2.44129007e-03 2.35913917e+00]\n",
      "pred = [4.34187417e-01 6.85921163e+00 1.55048519e-04 ... 1.32937674e+00\n",
      " 2.76638594e-03 2.77912982e+00]\n"
     ]
    }
   ],
   "source": [
    "tdp = TestDataProcessing()\n",
    "previous_revealed_targets = []\n",
    "N_day_lags = 7\n",
    "for (test, revealed_targets, client_test, historical_weather_test,\n",
    "     forecast_weather_test, electricity_test, gas_test, sample_prediction) in iter_test:\n",
    "    \n",
    "    # Rename test set to make consistent with train\n",
    "    test = test.rename(columns = {'prediction_datetime': 'datetime'})\n",
    "    \n",
    "    # Initiate column data_block_id with default value to merge the data on\n",
    "    id_column = 'data_block_id' \n",
    "    \n",
    "    test[id_column] = 0\n",
    "    gas_test[id_column] = 0\n",
    "    electricity_test[id_column] = 0\n",
    "    historical_weather_test[id_column] = 0\n",
    "    forecast_weather_test[id_column] = 0\n",
    "    client_test[id_column] = 0\n",
    "    revealed_targets[id_column] = 0\n",
    "    \n",
    "    data_test = tdp.feat_eng_test(test, client_test, historical_weather_test,\n",
    "                                  forecast_weather_test, electricity_test, gas_test, locations)\n",
    "    \n",
    "    data_test['datetime']= pd.to_datetime(data_test['datetime'], utc= True).astype('int64')\n",
    "    \n",
    "    # Store revealed_targets\n",
    "    previous_revealed_targets.insert(0, revealed_targets)\n",
    "    if len(previous_revealed_targets) == N_day_lags:\n",
    "        previous_revealed_targets.pop()\n",
    "    \n",
    "    # Add previous revealed targets\n",
    "    df_test = tdp.create_revealed_targets_test(data=data_test.copy(),\n",
    "                                               previous_revealed_targets=previous_revealed_targets.copy(),\n",
    "                                               N_day_lags=N_day_lags)\n",
    "    #Data Transformation\n",
    "    df_test['sin_hour']= (np.pi * np.sin(df_test['hour']) / 12)\n",
    "    df_test['cos_hour']= (np.pi * np.cos(df_test['hour']) / 12)\n",
    "    df_test['sin_hour']= (np.pi * np.sin(df_test['hour']) / 12)\n",
    "    df_test['cos_hour']= (np.pi * np.cos(df_test['hour']) / 12)\n",
    "    df_test['sin_dayofyear']= (np.pi * np.sin(df_test['dayofyear']) / 183)\n",
    "    df_test['cos_dayofyear']= (np.pi * np.cos(df_test['dayofyear']) / 183)\n",
    "    df_test = tdp.get_agg_target_lag(df_test)\n",
    "        \n",
    "    df_test = make_logs(df_test)\n",
    "    #for i in to_log:\n",
    "    #    df_test[f\"log_{i}\"]= np.where((df_test[i])!= 0, np.log(df_test[i]),0)\n",
    "    df_test = df_test.drop('currently_scored', axis= 1)\n",
    "    X_test = df_test.values\n",
    "    \n",
    "    print(f\"df_test the number of columns = {len(df_test.columns)}\\n {sorted(df_test.columns.tolist())} \")\n",
    "\n",
    "    #Predictions\n",
    "    #create a list to store predictions of each model\n",
    "    target_preds=[]\n",
    "    model_weights = [0.12, 0.13, 0.17, 0.16, 0.15, 0.14, 0.13]\n",
    "    pred=0\n",
    "    for i, model in enumerate(m_models):\n",
    "        pred = model.predict(X_test).clip(0)\n",
    "        print(f\"pred = {pred}\")\n",
    "        target_preds.append(pred)\n",
    "    \n",
    "    #weighted average\n",
    "    pred=0\n",
    "    for i in range(len(model_weights)):\n",
    "        pred += (target_preds[i]* model_weights[i])\n",
    "        \n",
    "    test['target'] = pred\n",
    "    \n",
    "#     #repeat above process for target_solar\n",
    "    tsolar_preds=[]\n",
    "    for i, model in enumerate(n_models):\n",
    "        pred = model.predict(X_test).clip(0)\n",
    "        print(f\"pred = {pred}\")\n",
    "        tsolar_preds.append(pred)\n",
    "        \n",
    "    pred_solar=0\n",
    "    for i in range(len(model_weights)):\n",
    "        pred_solar += (tsolar_preds[i]* model_weights[i])\n",
    "        \n",
    "    test['target_solar'] = pred_solar\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    test.loc[test['is_consumption']==0, \"target\"] = test.loc[test['is_consumption']==0, \"target_solar\"]  \n",
    "    sample_prediction[\"target\"] = test['target']\n",
    "    \n",
    "    #Sending predictions to the API\n",
    "    env.predict(sample_prediction)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    },
    {
     "datasetId": 4181108,
     "sourceId": 7223258,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4252362,
     "sourceId": 7326988,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 567.86461,
   "end_time": "2024-01-05T08:13:25.078067",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-05T08:03:57.213457",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
